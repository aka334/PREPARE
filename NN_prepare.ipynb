{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /mounts/u-amo-d1/grad/aka334/anaconda3/envs/cms/lib/python3.10/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /mounts/u-amo-d1/grad/aka334/anaconda3/envs/cms/lib/python3.10/site-packages (from lightgbm) (1.13.0)\n",
      "Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:00:31.916808: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 17:00:38.609840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 17:01:01.717349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/u/amo-d0/grad/aka334/anaconda3/envs/cms/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-12-06 17:01:19.673510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-06 17:01:19.679297: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 27992.5137 - mse: 27992.5137 - val_loss: 28455.2441 - val_mse: 28455.2441\n",
      "Epoch 2/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 26956.9023 - mse: 26956.9023 - val_loss: 24900.1836 - val_mse: 24900.1836\n",
      "Epoch 3/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 24805.3867 - mse: 24805.3867 - val_loss: 21829.9883 - val_mse: 21829.9883\n",
      "Epoch 4/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 21898.1895 - mse: 21898.1895 - val_loss: 20427.5723 - val_mse: 20427.5723\n",
      "Epoch 5/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 18827.4102 - mse: 18827.4102 - val_loss: 17457.1855 - val_mse: 17457.1855\n",
      "Epoch 6/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 15903.2129 - mse: 15903.2129 - val_loss: 13416.3086 - val_mse: 13416.3086\n",
      "Epoch 7/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 12902.6055 - mse: 12902.6055 - val_loss: 10081.8652 - val_mse: 10081.8652\n",
      "Epoch 8/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 9546.3271 - mse: 9546.3271 - val_loss: 7882.5259 - val_mse: 7882.5259\n",
      "Epoch 9/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 6855.2651 - mse: 6855.2651 - val_loss: 4293.0063 - val_mse: 4293.0063\n",
      "Epoch 10/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 4666.1226 - mse: 4666.1226 - val_loss: 3175.3113 - val_mse: 3175.3113\n",
      "Epoch 11/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 3304.5200 - mse: 3304.5200 - val_loss: 2692.5720 - val_mse: 2692.5720\n",
      "Epoch 12/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 2444.7451 - mse: 2444.7451 - val_loss: 1973.8857 - val_mse: 1973.8857\n",
      "Epoch 13/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 1889.5225 - mse: 1889.5225 - val_loss: 2096.9407 - val_mse: 2096.9407\n",
      "Epoch 14/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 1727.4656 - mse: 1727.4656 - val_loss: 1949.7870 - val_mse: 1949.7870\n",
      "Epoch 15/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 1561.3910 - mse: 1561.3910 - val_loss: 2089.8103 - val_mse: 2089.8103\n",
      "Epoch 16/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 1464.1821 - mse: 1464.1821 - val_loss: 1791.0337 - val_mse: 1791.0337\n",
      "Epoch 17/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 1411.0891 - mse: 1411.0891 - val_loss: 2101.6323 - val_mse: 2101.6323\n",
      "Epoch 18/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 1362.7385 - mse: 1362.7385 - val_loss: 2087.6575 - val_mse: 2087.6575\n",
      "Epoch 19/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 1343.2943 - mse: 1343.2943 - val_loss: 1999.4814 - val_mse: 1999.4814\n",
      "Epoch 20/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 1420.2975 - mse: 1420.2975 - val_loss: 2040.0100 - val_mse: 2040.0100\n",
      "Epoch 21/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 1260.8445 - mse: 1260.8445 - val_loss: 1882.0416 - val_mse: 1882.0416\n",
      "Epoch 22/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 1272.9358 - mse: 1272.9358 - val_loss: 1804.1249 - val_mse: 1804.1249\n",
      "Epoch 23/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 1245.0156 - mse: 1245.0156 - val_loss: 1698.0276 - val_mse: 1698.0276\n",
      "Epoch 24/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 1314.1172 - mse: 1314.1172 - val_loss: 1573.7379 - val_mse: 1573.7379\n",
      "Epoch 25/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 1311.1398 - mse: 1311.1398 - val_loss: 1609.7596 - val_mse: 1609.7596\n",
      "Epoch 26/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1252.9797 - mse: 1252.9797 - val_loss: 1602.6775 - val_mse: 1602.6775\n",
      "Epoch 27/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 1216.0702 - mse: 1216.0702 - val_loss: 1605.3750 - val_mse: 1605.3750\n",
      "Epoch 28/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1254.1903 - mse: 1254.1903 - val_loss: 1612.9155 - val_mse: 1612.9155\n",
      "Epoch 29/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 1162.6169 - mse: 1162.6169 - val_loss: 1572.9943 - val_mse: 1572.9943\n",
      "Epoch 30/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 1251.4119 - mse: 1251.4119 - val_loss: 1625.2142 - val_mse: 1625.2142\n",
      "Epoch 31/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 1126.5549 - mse: 1126.5549 - val_loss: 1626.1284 - val_mse: 1626.1284\n",
      "Epoch 32/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1172.6896 - mse: 1172.6896 - val_loss: 1600.8021 - val_mse: 1600.8021\n",
      "Epoch 33/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 1221.3190 - mse: 1221.3190 - val_loss: 1614.0725 - val_mse: 1614.0725\n",
      "Epoch 34/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1083.2997 - mse: 1083.2997 - val_loss: 1564.2195 - val_mse: 1564.2195\n",
      "Epoch 35/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 1100.5383 - mse: 1100.5383 - val_loss: 1577.9686 - val_mse: 1577.9686\n",
      "Epoch 36/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 1126.3418 - mse: 1126.3418 - val_loss: 1571.6368 - val_mse: 1571.6368\n",
      "Epoch 37/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 1140.7709 - mse: 1140.7709 - val_loss: 1550.8250 - val_mse: 1550.8250\n",
      "Epoch 38/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 1140.7599 - mse: 1140.7599 - val_loss: 1543.0743 - val_mse: 1543.0743\n",
      "Epoch 39/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 1122.1417 - mse: 1122.1417 - val_loss: 1521.0806 - val_mse: 1521.0806\n",
      "Epoch 40/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 1086.2616 - mse: 1086.2616 - val_loss: 1508.7338 - val_mse: 1508.7338\n",
      "Epoch 41/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 1092.4862 - mse: 1092.4862 - val_loss: 1553.3809 - val_mse: 1553.3809\n",
      "Epoch 42/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1075.0021 - mse: 1075.0021 - val_loss: 1528.7743 - val_mse: 1528.7743\n",
      "Epoch 43/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1077.2512 - mse: 1077.2512 - val_loss: 1532.9563 - val_mse: 1532.9563\n",
      "Epoch 44/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 1001.9879 - mse: 1001.9879 - val_loss: 1529.2910 - val_mse: 1529.2910\n",
      "Epoch 45/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 931.2472 - mse: 931.2472 - val_loss: 1533.4473 - val_mse: 1533.4473\n",
      "Epoch 46/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 1027.3903 - mse: 1027.3903 - val_loss: 1551.3131 - val_mse: 1551.3131\n",
      "Epoch 47/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 979.9111 - mse: 979.9111 - val_loss: 1550.8749 - val_mse: 1550.8749\n",
      "Epoch 48/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 1030.9437 - mse: 1030.9437 - val_loss: 1595.2959 - val_mse: 1595.2959\n",
      "Epoch 49/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 985.0292 - mse: 985.0292 - val_loss: 1548.0144 - val_mse: 1548.0144\n",
      "Epoch 50/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 966.7797 - mse: 966.7797 - val_loss: 1591.5623 - val_mse: 1591.5623\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Validation RMSE: 38.84241937606702\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step\n",
      "Submission file saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load datasets\n",
    "submission_format = pd.read_csv('./submission_format.csv')\n",
    "test_features = pd.read_csv('./test_features.csv')\n",
    "train_features = pd.read_csv('./train_features.csv')\n",
    "train_labels = pd.read_csv('./train_labels.csv')\n",
    "\n",
    "# Step 1: Merge train features and labels\n",
    "train_data = train_labels.merge(train_features, on='uid')\n",
    "train_data[\"pred_year\"] = train_data[\"year\"] - 2012\n",
    "\n",
    "# Step 2: Align test features with submission format\n",
    "aligned_test_features = submission_format[[\"uid\", \"year\"]].merge(test_features, on=\"uid\")\n",
    "aligned_test_features[\"pred_year\"] = aligned_test_features[\"year\"] - 2012\n",
    "\n",
    "# Step 3: Prepare features and labels\n",
    "X = train_data.drop(columns=['uid', 'year', 'composite_score'])\n",
    "y = train_data['composite_score']\n",
    "\n",
    "# Step 4: Handle missing values\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "aligned_test_features[num_cols] = num_imputer.transform(aligned_test_features[num_cols])\n",
    "aligned_test_features[cat_cols] = cat_imputer.transform(aligned_test_features[cat_cols])\n",
    "\n",
    "# Step 5: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    aligned_test_features[col] = le.transform(aligned_test_features[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Step 6: Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "aligned_test_features[num_cols] = scaler.transform(aligned_test_features[num_cols])\n",
    "\n",
    "# Step 7: Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Build Neural Network\n",
    "def build_nn(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "nn_model = build_nn(X_train.shape[1])\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Step 9: Train Neural Network\n",
    "history = nn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 10: Evaluate Model\n",
    "y_pred = nn_model.predict(X_val).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Validation RMSE: {rmse}\")\n",
    "\n",
    "# Step 11: Predict on Test Data\n",
    "X_aligned_test = aligned_test_features.drop(columns=['uid', 'year'])\n",
    "aligned_test_predictions = nn_model.predict(X_aligned_test).flatten()\n",
    "\n",
    "# Step 12: Prepare Submission File\n",
    "submission = submission_format.copy()\n",
    "submission['composite_score'] = np.round(aligned_test_predictions).astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved to: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
